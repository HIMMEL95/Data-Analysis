{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a태그의 href 속성을 리스트로 추출하여, 크롤링 할 페이지 리스트를 생성합니다.\n",
    "page_url_base = \"https://namu.wiki\"\n",
    "page_urls = []\n",
    "for index in range(0, len(table_rows)):\n",
    "    first_td = table_rows[index].find_all('td')[0]\n",
    "    td_url = first_td.find_all('a')\n",
    "    if len(td_url) > 0:\n",
    "        page_url = page_url_base + td_url[0].get('href')\n",
    "        if 'png' not in page_url:\n",
    "            page_urls.append(page_url)\n",
    "\n",
    "# 중복 url을 제거합니다.\n",
    "page_urls = list(set(page_urls))print(contents_table)\n",
    "for page in page_urls[:5]:\n",
    "    print(page)"
   ]
  },
  {
   "source": [
    "# 질문 내용\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed()\n",
    "\n",
    "# 여기에 들어가게 되는 숫자는?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 크롤링할 사이트 주소를 정의합니다.\n",
    "source_url = \"https://www.naver.com/\"\n",
    "\n",
    "# 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "req = requests.get(source_url)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "contents_table = soup.find(name=\"span.keyword\")\n",
    "#table_body = contents_table.find(name=\"a\")\n",
    "#table_rows = table_body.find_all(name=\"span.keyword\")\n",
    "\n",
    "# print(table_rows, \".\", table_body)\n",
    "\n",
    "print(contents_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 태풍미탁 \n 기온 \n 소한 \n 2020년장마기간 \n 대구날씨 \n 금요일날씨 \n 크리스마스 \n 수원날씨 \n 날아라슛돌이 \n 동지 \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 크롤링할 사이트 주소를 정의합니다.\n",
    "source_url = \"https://search.naver.com/search.naver?sm=tab_hty.top&where=nexearch&query=%EB%82%A0%EC%94%A8&oquery=%ED%8E%9C%ED%8A%B8%ED%95%98%EC%9A%B0%EC%8A%A4+%EB%82%98%EB%B9%84%EB%AC%B8%EC%8B%A0&tqi=U%2BbcJlprvmsssLqawa0ssssst5s-342613\"\n",
    "\n",
    "# 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "req = requests.get(source_url)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "contents_table = soup.find('div',class_='sub_pack')\n",
    "#contents_table = soup.find(name=\"div.weather_box\")\n",
    "table_body = contents_table.find(\"div\", class_='api_subject_bx _related_box')\n",
    "table_rows = table_body.find_all(\"a\", class_='keyword')\n",
    "\n",
    "for title in table_rows:\n",
    "    print(title.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 정일훈\n2 2021 수능 등급컷\n3 예수비전교회\n4 비투비\n5 펜트하우스 나비문신\n6 진성준\n7 구하라 재산\n8 1가구 1주택\n9 싱어게인 30호 이승윤\n10 스키장 폐쇄\n11 국제수사\n12 싱어게인 30호\n13 고려진\n14 전봉민\n15 금천구 예수비전교회\n16 코스닥\n17 수능 성적 발표\n18 권덕철\n19 은수미\n20 비긴어게인\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 크롤링할 사이트 주소를 정의합니다.\n",
    "source_url = \"https://search.naver.com/search.naver?where=nexearch&query=%EC%A0%95%EC%9D%BC%ED%9B%88&sm=top_lve.ag20sgrpma0si0en0sp0&ie=utf8\"\n",
    "\n",
    "# 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "req = requests.get(source_url)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "contents_table = soup.find('div', class_='sub_pack')\n",
    "#contents_table = soup.find(name=\"div.weather_box\")\n",
    "table_body = contents_table.find('div', class_='api_subject_bx')\n",
    "table_rows = table_body.find_all(\"a\", class_='keyword')\n",
    "\n",
    "count = 1\n",
    "for title in table_rows:\n",
    "    print(count, title.text)\n",
    "    count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}